\chapter{Animation}
\label{chapter_animation}

The animation in the system consists of moving the virtual avatar and the cloth mesh in a realistic manner, with the addition of physical simulaton of the apparel meshes. This chapter focuses on the animation of human avatar and the static cloth meshes, the physical animation is explained in Chapter~\ref{chapter_cloth_simulation}.

The rigged human body mesh ready to be animated is exported from Blender in Ogre XML format. The XML files are converted to binary skeleton and mesh files by the OgreXML serializer and imported natively into the software. Prior to animation, the bones are set up to be updated by the data from depth sensor. Afterwards, they are updated with the orientation data.

\section{Initialization}
\label{section_animation_initialization}

Animatable meshes are loaded and maintained via the custom \em{SkeletalMesh} class, which is the middleware between Ogre skeletal animating system and the input from Kinect sensor. An instance of the \em{SkeletalMesh} class is initialized with a mesh file. After the mesh is loaded with the skeleton data, the bone list is iterated through, given the initial orientation uniquely for each animatable bone with the Kinect sensor. 

The animatable bones are set to be manually controlled and given the initial orientations. The lower part of the body inherits orientations as they are filtered for footskating and use different orientation mechanisms. The upper part of the body is rotated in global space. The bones are set to initial state, to be reset at every frame and updated with new orientations. The non-animated bones are left to be automatically controlled in order to keep them aligned with their parent bones. 

\section{Animation}
\label{section_animation}
Every frame, the orientation information from the bones are extracted in quaternion form, along with the confidence of the sensor in that orientation. If the confidence is less than 0.5 over 1, the bones are left as they were in the previous frame to avoid unnatural movements. The new orientation is fed into the bone after it is multiplied with the initial orientation. The root bone is translated in local space at the end to simulate the translation above rotation. This technique is used with the static parts of the apparel meshes as well. The linear weighted skin blending~\cite{Kavan2003} is used in order to simulate deformation on the characters skin. The effects of four bones with different weights are combined linearly to change the positions and normal of vertices. 

The algorithm in Algorithm~\ref{algo:transformBone} is executed during the pre-render cycle. The bone orientations are set to be ready for animation, deformation and rendering. At every render cycle, update Skeleton function is called, which automatically fetches the coordinates from the Kinect and sets up the skeleton for vertex blending. 

\begin{algorithm}[ht]
\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
\SetKwBlock{function}{function}{endfunction}
\function(transformBone\ArgSty{(bone)}){
\KwIn{A bone and the corresponding orientation matrix from Kinect}
\KwOut{The same bone with updated orientation}
$q_I =$ initial orientation of bone\;
$q_N =$ relative orientation\;
$q_K = 3\times3$ Orientation matrix from Kinect\;
\If{\textit{kinect}$_\textit{confidence} > 0.5$} {
  $q_Q$ = \textit{toQuaternion}(q$_K$)\;
  $q_N$ = \textit{toLocalSpace}(q$_Q$)\;
  $q = q_N \times q_I$\;
  $\textit{bone.orientation} = \textit{q.normalise}{\left(\right)}$\; }}

\If{\textit{user} {\bf is } \textit{new}} {
  $p_\textit{torso}.\textit{initialize}()$\tcc*{Initialize torso position}  \;
 }
\ForEach {bone}{
\If{${\textit{bone}}_{\textit{orientation}}$ ${\bf is }$ \textit{new}}{
\textit{transformBone(bone)}\;
\textit{skeleton.needsUpdate()}\;
}
}
\caption{Bone transformation algorithm}
\label{algo:transformBone}
\end{algorithm}

Algorithm \ref{algo:updateMesh} is executed prior to rendering, to update the vertices of the skin. The vertex blending function is where the deformation
actually occurs. Every vertex is assigned to at most four different bones. In order to speed up the deformation process, the transformation matrices for the corresponding matrices are collapsed into one. Collapsing is simply weighted addition of the four weighting matrices for a vertex.


\begin{algorithm}[ht]
\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
\SetKwBlock{function}{function}{endfunction}
\function(prepareBlendMatrices\ArgSty{(mesh)}){
\ForEach {bone}{
bone.applyScale()\;
bone.applyTransform()\;
bone.applyOrientation()\;
}
$i=0$\;
\ForEach {bone}{
$m[i]$ = \textit{bone.getTransformationMatrix} $4 \times 4$\;
$i++$\;
}
\tcc*{Index map contains the bone pointers for every vertex}\;
mapIndex=mesh.getIndexMap() 
$i=0$\;
\tcc*{Blend matrices are pointers to the individual bone matrices}\;
\ForEach {indexSet ${\bf in }$ mapIndex}{
$m_b[i] = \textit{indexSet}[i]+m$ 
$i++$\; }
\Return $m_b$\;
}

\function(vertexBlend\ArgSty{($m_b$)}){
pos=*mesh.positions\tcc*{Pointer to position matrix}\;
norm=*mesh.normals\tcc*{Pointer to normal matrix}\;
$b_i=$*mesh.blendIndices\tcc*{Pointer to blend index matrix}\;
$b_w=$*mesh.blendWeights\tcc*{Pointer to blend weight matrix}\;
\ForEach {4 vertices ${\bf in }$ pos}{
\ForEach {vertex ${\bf in }$ 4 vertices}{ 
$m[1,2,3,4]=m_b[b_i[vertex]]$ \tcc*{Weighting Bones}\;
$m_c[j]$ = \textit{collapseMatrix(m, b$_w[vertex]$)}{\bf (i)}\;
}
$pos[4 vertex]=m_c\times pos[4 vertex]$
$norm[4 vertex]=m_c\times norm[4 vertex]$
}}
\If {skeleton needs update}{
$m_b$ = \textit{prepareBlendMatrices(skeleton.mesh)}\;
\textit{vertexBlend}(m$_b$)\;
}
\caption{Mesh update algorithm called at every frame.}
\label{algo:updateMesh}
\end{algorithm}

%%Bir sonraki cumle yeniden yazilmali....
After four weighting matrices for each of four vertices are collapsed into four matrices, the following matrix operations are performed. Four vertices are processed together in order to fully utilize the parallel matrix multiplication features. Let us take the weighted matrix for vertex $i$:

\begin{equation}
M_i=
\begin{bmatrix}
m_{00}^i & m_{01}^i & m_{02}^i & m_{03}^i \\
m_{10}^i & m_{11}^i & m_{12}^i & m_{13}^i \\
m_{20}^i & m_{21}^i & m_{22}^i & m_{23}^i \\
0 & 0 & 0 & 1
\end{bmatrix}
\label{eqn:weighted_matrix_for_i}
\end{equation}

It should be noted that this matrix is a linear combination of four linear transformation matrices from four weighting bones; hence, the $3^{rd}$ row is by default [0 0 0 1]. We also have the initial positions for the four vertices as $[p^i_x \: p^i_y  \: p^i_z  \: 1]$.$3^{rd}$ value, which is one is not included in the vertex buffer and it will not be taken into account with the calculations. In order to generate an efficient SIMD process, we perform 4$\times$3 dot product calculations in each instruction. Dot products are commanded in the machine language, which makes it more efficient to higher level matrix calculations. To compute the simulated x coordinates for  four vertices, we combine the first rows of all collapsed matrices into a 4 $\times$ 4 matrix and transpose it:

\begin{equation}
M_T=
\begin{bmatrix}
m_{00}^0 & m_{00}^1 & m_{00}^2 & m_{00}^3 \\
m_{01}^0 & m_{01}^1 & m_{01}^2 & m_{01}^3 \\
m_{02}^0 & m_{02}^1 & m_{02}^2 & m_{02}^3 \\
m_{03}^0 & m_{03}^1 & m_{03}^2 & m_{03}^3 
\end{bmatrix}
\label{eqn:transposed_weight_matrix}
\end{equation}

We construct a position matrix of size 4 $\times$ 3 with the positions of all vertices:

\begin{equation}
P_T=
\begin{bmatrix}
p_{x}^0 & p_{x}^1 & p_{x}^2 & p_{x}^3 \\
p_{y}^0 & p_{y}^1 & p_{y}^2 & p_{y}^3 \\
p_{z}^0 & p_{z}^1 & p_{z}^2 & p_{z}^3  
\end{bmatrix}
\label{eqn:transposed_position_matrix}
\end{equation}

Next, the matrices $M_T$ and $P_T$ are multiplied in a row-by-row fashion and summed together to calculate the x-coordinate displacements
of four vertices:

\begin{equation}
d_x=M_{T0} \times P_{T0} + M_{T1} \times P_{T1} M_{T2} \times P_{T2} +M_{T3}
\label{eqn:collapsed_matrix}
\end{equation}

The result vector $d_x$ is a $4 \times 1$ vector, which has the post-blended vertex x-coordinates: $[P^0_x \: P^1_x  \: P^2_x  \: P^3_x]$. The same procedure is applied to the normal of a vertex. Process continues with the next set of four vertices. 

\section{Interaction Between the Body and Cloth}
\label{section_body_cloth_interaction}
The movements of the user are passed on the pieces of cloth separately. Non simulated parts of the clothes are the ones which do not get separated from the body most of the time. Most parts of our clothes usually stick to the body and experience the same deformation as our skin. In order to increase performance and get better results, detailed simulation on these parts are not run, instead they are deformed the same as the remained of human body. For instance, in the full-body dress mesh, the part above the waist has a skeleton similar to the body and the information from the Kinect is passed on to this portion as well. It is treated as a part of the actual avatar.

The movements of the body are transferred into the simulated section of the cloth through transformation, collision and inertia. The fixed vertices are transformed to match the remainder of the cloth. The transformation is done on the rendering level, the physx world experiences no difference in transformation manner. This process keeps the cloth aligned with the rest of the visible world. The colliding body is updated and collided with the cloth. The colliding body consists of 16 spheres and the capsules connecting the spheres. The details of the colliding body is explained in Section \ref{section_human_avatar}. This process keeps the cloth out of the avatar's way. The Inertia of transformations is passed onto the simulated cloth, increasing the realism. The passed on inertia comes from the rotation and the transformation of the root bone of the human skeleton. With this process, although there is no actual transformation in the physics world, the resulting inertia effects are visible on the cloth itself. 

\section{Motion Filtering}
\label{section_motion_filtering}

Application of the raw data from the sensor causes unnatural movements due to the noise in the sensor input, self-occlusions of the body and inadequate IK solvers. In order to present a more realistic avatar animation, a series of filters and constraints are applied to the sensor data. 

\subsection{Position Filtering}
The most severe disruption of the self-occlusion problem takes place in the joint position acquisition. There is no possible way of acquiring the correct position of a limb when the sensor has no vision of it. However, the way humans move their limbs under normal conditions follow certain principles and trends, which can be used to estimate the locations of occluded body parts. The nature of these motions, demonstrating traits similar to seasonal behavior, makes them suitable for applying a variety of filters~\cite{Azimi2012}. The framework utilizes the Holt-Winters double exponential smoothing~\cite{Holt2004,Kalekar2004}  as it comes with the middleware, easy to use and delivers good quality results with acceptable latency for the purposes of this application. 

\subsection{Rotation Filtering and Constraints}
The joint orientations are acquired from the sensor middleware directly, however the data is not smooth. Although the middleware enforces certain constraints (such as allowing only pitch rotations on ulna), there are often significant gaps in the estimated angles that produce unnatural tremor-like movements on the avatar. Furthermore, there is no filtering of unnatural rotations that take place when an occluded body part is estimated to be in a wrong location. 

The inferior quality of the orientation data is improved in two stages: applying another set of constraints on the joint data based on the natural limits of human bones, followed by an asymptotic smoothing of the joint angles to prevent the effect of gaps in the angles (see~Figure~\ref{fig:rotation-filter}). 

\begin{figure}[htbp]
	\centerline{ 
	\fbox{\includegraphics[width=0.98\textwidth]{./figures/rotation-filter.eps}}
	}
\caption{The row and filtered samples for right the humerus roll angle}
	\label{fig:rotation-filter}
\end{figure}


\subsection {Bone Splitting}
\label{subsection_bone_splitting}
The lower sections of human limbs contain two parallel bones allowing the twisting rotation on the hands and feet. The configurations of bones allow all portions of the lower limbs follow the bones in pitch and roll rotations, although the effect of yaw rotation decreases as it gets closer to the mid-section joint (elbow or knee). This effect is not possible to achieve with a single bone fore-arm representation, as specified in the highest level of detail in the H-ANIM standard~\cite{HANIM}, using unified weights (same for all types of rotations, transformations and scaling) and linear skinning. On the other hand, applying a different set of weights for each possible  transformation, rotation or scaling requires additional space and time, which can be considered redundant as it is not going to be used in most parts of the skeleton and surface mesh; thus, it is not implemented in most of the popular rendering engines. This problem is addressed by Kavan et al.~\cite{Kavan2009}, proposing a method of introducing additional blending bones to simulate non-linear skinning. However, this approach is not suitable for a real-time application with previously unknown motions. 

Since the only problematic bones for this particular case are the upper limbs, this problem is solved using a novel approach by introducing an additional bone connected in series for the upper limbs. The ulna bones are split halfway and the lower sections are labeled as ulna-extension. The vertex weights in the corresponding sections are divided linearly among two sections, as seen in Figure~\ref{fig:forearm-weights}. 


\begin{figure}[htbp]
	\centerline{\includegraphics[width=0.75\textwidth]{./figures/ulna-weight.eps}}
	\centerline{(a)}
	\centerline{\ }
	\centerline{\includegraphics[width=0.75\textwidth]{./figures/ulna-extent-weight.eps}}
	\centerline{(b)} 
	\centerline{\ } 
	\caption{The vertex weights for (a) the upper ulna and (b) the lower ulna bone (weight increases from blue to yellow).}
	\label{fig:forearm-weights}
\end{figure}

During runtime, the filtered local rotation of the upper limb bones are separated into two distinct rotations, one containing the yaw and the other containing pitch and roll rotations, which are applied to the extension bone and the original bone, respectively. With proper weights, the rotation of the users arm is transferred to the virtual avatar naturally without introducing any artifacts. As seen in Figure~\ref{fig:forearm-comparison}, the vertices on the forearm twist in a more natural way resembling the real motion.

\begin{figure}[htbp]
	\centerline{\includegraphics[width=1.0\columnwidth]{./figures/fore-arm-single-bone.eps}}
	\centerline{(a)}
	\centerline{\ }
	\centerline{\includegraphics[width=1.0\columnwidth]{./figures/fore-arm-double-bone.eps}}
	\centerline{(b)}
	\caption{Comparison of a -90$^\circ$ yaw rotation on the forearm with: (a) single and (b)~double-boned skinning.}
	\label{fig:forearm-comparison}
\end{figure}

\section{Handling the Foot Skating Problem}
\label{section_foot_skating}
The virtual model is animated by changing bone orientations and root bone position. If the orientations and root position are applied to the bones respectively, the feet of the character appear to be floating on the ground which deteriorates the realism of the system. 
This issue, known as footskating is not limited to depth sensor applications. It has been detected in motion capture systems and solved sufficiently with approaches by Kovar et al.~\cite{Kovar2002} and Ikemoto et al.~\cite{Ikemoto2006}. However, the proposed methods in these approaches rely on some sort of preprocessing or supervised learning, which is not suitable for our system, as the motion is captured and applied im real time. Hence, a solution which does not require a training process is needed for such applications. Mankyu and Choi~\cite{Mankyu2013} proposed a similar method which does not require training for real-time depth sensor applications. Their approach is adopted into our framework to overcome the footskating problem. However, it required additional filters and constraints in order to be usable in extended animations. It is assumed that one of the feet is always on the ground, artifacts may occur if the user attempts jumping. The whole footskating algorithm consists of five steps. 

\begin{enumerate}
\item Determine which foot is constrained by checking speed and location thresholds. The thresholds are changed adaptively in order to compensate for different sensor and user placements.

\item Place the constrained foot at its last recorded position and solve the inverse kinematics problem to determine the orientations of the hip and knee joints. Solving an inverse kinematic problem is no trivial task. There are various numerical and analytical methods for inverse kinematic problems and many implementations focusing on each. For our framework, we embedded the IKAN library by University of Pennsylvania~\cite{IKAN2013}, which uses the approach described in~\cite{Tolani2000}.  

\item Check if the foot coincides with its intended position after the inverse kinematic orientations are applied to the bones. If there is a positional mismatch, relocate the root joint to complete alignment.

\item Smooth the joint orientation difference in order to avoid jumps from frame to frame. This is especially important when the constraint switches, as the source of the orientations are different in two cases and they do not always overlap. Smoothing parameters are optimized in a try-and-error basis, they are not viable for adaptive nature. Smoothing also helps to overcome the self-occlusion and data-noisiness.
\end{enumerate}

The overall algorithm consists of two major parts: {\em constraint checking and adaptation} (cf.~Algorithm~\ref{algo:check_foot_constraints}) and {\em joint angle determination and application} (cf.~Algorithm~\ref{algo:foot_skating}).  

\begin{algorithm}[ht]
\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
\If{\textit{isFirstFrame}}{
\If{$p^y_{left} < p^y_{right}$ }{
	\textit{constrained = left}\;
	$y_\textit{threshold}=p^y_\textit{left}$\;
}
\Else {
	\textit{constrained = right}\;
	$y_\textit{threshold}=p^y_\textit{right}$\;
}
}

\If{$constrained is left$}{
	\tcc{Check if constraints still hold}
	\If{$p^y_\textit{left} > y_\textit{threshold} \;\; and \;\; v_\textit{left} > v_\textit{threshold}$}{
		\tcc{Foot is not constrained anymore, check other foot}
		\If{$p^y_\textit{right} > y_\textit{threshold} \;\;and\;\; v_\textit{right} > v_\textit{threshold}$}{
			\tcc{Neither foot are constrained}
			\textit{updateThresholds()} \tcc*[f]{Thresholds should be updated} 
			\textit{restart}\;
		}
		\Else(\tcc*[f]{Right foot is constrained, feet switched}){		
			\textit{constrained=right}\;
			\textit{recordRightFootPosition()}\;
			\textit{constraintSwitched = True}\;
		}
	} 
}
\Else{ 
	\tcc{Check if constraints still hold} 
	\If{$p^y_{\textit{right}} > y_{\textit{threshold}} \;\;and\;\; v_{\textit{right}} > v_{\textit{threshold}}$}{
		\tcc{Foot is not constrained anymore, check other foot}
		\If{$p^y_\textit{left} > y_\textit{threshold} \;\;and\;\; v_\textit{left} > v_\textit{threshold}$}{
			\tcc{Neither foot are constrained}
			\textit{updateThresholds()} \tcc*[f]{Thresholds should be updated} 
			\textit{restart}\;
		} 
		\Else (\tcc*[f]{Left foot is constrained, feet switched}){	
			\textit{constrained = left}\;
			\textit{recordRightFootPosition()}\;
			\textit{constraintSwitched = True}\;
		}
	} 
}
\caption{Constrained foot determination}
\label{algo:check_foot_constraints}
\end{algorithm}

 
\begin{algorithm}[ht]
\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
{\textit checkFootConstraints()} \tcc*[f]{Check which foot is constrained}
$p^{\textit{foot}}_{\textit{constrained}} = {\textit getRecordedFootPosition()}$\;
${\textit{target}}_{ik} = p^{\textit{foot}}_{\textit{constrained}} - p^{\textit{hip}}_{\textit{constrained}}$\;
\tcc{Get the rotation quaternions for constrained hip and knee}
$q_{\textit{hip}}, q_{\textit{knee}}$ =  \textit{solveIkan}(\textit{constrained}, {\textit{target}}$_{ik}$)\;
${\textit{hip}}_{\textit{constrained}}.{\textit{rotate}}(q_{\textit{hip}})$\;
${\textit{foot}}_{\textit{constrained}}.{\textit{rotate}}(q_{\textit{hip}})$\;
$p^{\textit{foot}}_{\textit{new}} = {\textit{calculateForwardKinematics()}}$\;
\tcc{Calculate the root displacement vector}
$d_{{\textit{root}}}$ = $p^{\textit{foot}}_{\textit{constrained}}$ - $p^{\textit{foot}}_{\textit{new}}$\;
\textit{root}.\textit{translate}(d$_{\textit{root}}$)\;
\tcc{Proceed with joint smoothing}
\caption{Foot skating filtering}
\label{algo:foot_skating}
\end{algorithm}

Joint smoothing consists of interpolating between frames, especially when there is a significant change in orientations, which would normally cause a non-continuous animation. Old joint orientations are always recorded in global coordinate system and updated everyframe. After we get the new orientation from the new frame, we calculate the quaternion which would rotate the old orientation to the new one:

\begin{equation}
Q_{d} = Q_{new} \times Q_{old}^{-1}
\label{eqn:rotator_quaternion}
\end{equation} 

When we get te delta quaternion, we get its axis-angle representation. Because we are interested in rotating the joint partially, we build up a new quaternion with the same axis and a fraction of the same angle. The fraction varies with whether the constraint is switched recently. 

\begin{equation} 
\begin{split}
Q_{d} = \textit{Quaternion}((x, y, z), \alpha) \\  
Q_d^* = \textit{Quaternion}((x, y, z), \alpha / k )
\label{eqn:partial_rotator}
\end{split}
\end{equation} 

The fractional rotations are then applied to the joints to get a smoother movement. Other implicit operations include coordinate system transformations, as we are working with three different coordinate systems: Global-Render Coordinate System, Local-Render Coordinate System, and IKAN Coordinate System. The constraints of the IKAN library require certain to-and-from transformations to be used with our coordinate system. 