\chapter{Cloth Resizing}
\label{chapter8}
My cloth resizing algorithm consists of three main steps:
\begin{enumerate}
\item Improve the raw depth map from Kinect by filtering.
\item Fit the contour on the user blob and perform measurements. Compare with known human body proportions to acquire required scaling parameters.
\item Perform the same measurements over a time frame in order to smooth the results. Scale the virtual avatar along with the cloth mesh prior to simulation.
\end{enumerate}

\section{Depth Map Optimization}
\label{section_8_1}

At 30 fps, Kinect provides a depth map and a user map, both at 640x480 resolution. Depth map consists of distances with the sensor in millimeters. 

The depth measurement of the Kinect is not very accurate compared to high end 3D
depth systems like laser scanners. The accuracy of the depth value decreases
quadratically. Error values for different distances is shown in Table
\ref{tbl:error_kinect}.

\begin{table}
\center
\begin{tabular}{ | l | l | l | l |}
\hline
\textbf{Distance of Point} & 1m & 3m & 5m \\ \hline
\textbf{Error in measurement} & 0.5cm & 1.5cm & 4cm \\ 
\hline
\end{tabular}
\caption{Kinect Depth Accuracy \cite{Kourosh2012}}
\label{tbl:error_kinect}
\end{table}

For my application, the Kinect needs to be able to see the whole human body,
which requires at least 3m away from the sensor for a person with 1.7m height,
resulting in an erroneous depth map. This problem can be seen in Figure
\ref{fig:kinect_depth_output_comparison}.


\begin{figure}[h]
\centerline{\fbox{\psfig{figure=figures/kinect_depth_output_comparison.png,width=1.00\textwidth}}}
\caption{Depth Output From Kinect with Improved Results. The leftmost image is
the raw output, the application of filters results in better performance in the other two images  \cite{Tong2012}. This improvement utilizes more than one depth streams. }
\label{fig:kinect_depth_output_comparison}
\end{figure}

In order to acquire a better depth map, I will perform the following operations:

Let us take the depth map D as a 640x480 matrix. Initially, the user pixels are
extracted by a pixel-by-pixel comparison with the user map. User map is another
acquired map from the sensor, with the same size as depth map. The value of a
pixel is set to a non-zero value if the pixel belongs to a recognized user. In
this case, we are only interested in one user, $D_1$ represents the depth pixels
of the user we are interested in, whereas $U_1$ is the bit map of the user.
Also, the non-user pixels must be filled with the mean value of the user pixels, in order to perform Gaussian filtering on the image.

\begin{equation}
D_1=(D-(D \times U_1 )) \times 1/n \times \sum\limits_{i=0}^n ((D \times U_1 )_i + d \times U_1 )
\label{eqn:patch_depth}
\end{equation}

Next, we perform Gaussian filtering on the userâ€™s depth map, to normalize and
improve the quality of the depth map. The size and sigma parameters of the Gaussian filter will be varied in order to maximize the performance and the quality of the results. 

\begin{equation}
D_G=D_1*G
\label{eqn:gaussian_convolution}
\end{equation}

After these operations, we have a normalized and filtered depth map, which also
has better planar values (x and y) since the holes due to depth stream will be filled.

\begin{algorithm}
\dontprintsemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
\KwIn{Raw Depth Stream From Kinect}
\KwOut{Depth Stream With Patched Holes and Gaussian Optimization }
$depth_{sum}=0$ \;
$n_{user} =0$\;
\For{i \bf{from} 0 \bf{to} $d_width$ }{
\For{j \bf{from} 0 \bf{to} $d_height$ }{
\If{$U(i,j)$} {
  $depth_{sum}=depth_{sum}+D(i,j)$\;
  $n_{user}+=1$\;
 }}}
$depth_{average}=depth_{sum}/n_{user}$ \;
\For{i \bf{from} 0 \bf{to} $d_width$ }{
\For{j \bf{from} 0 \bf{to} $d_height$ }{
\If{\bf{not}  $U(i,j)$} {
  $D(i,j)=depth_{average}$\;
 }}}
 
\For{i \bf{from} 0 \bf{to} $d_width$ }{
\For{j \bf{from} 0 \bf{to} $d_height$ }{
\If{$U(i,j)$} {
  $D(i,j)=D(i-m:i+m,j-n:j+m) * Gaussian(m,n,e)$\;
 }}}
\Return{D}
\caption{Depth Map Optimization Algorithm}
\label{algo:depth_patch}
\end{algorithm}

\section{Parameter Measurement}
In paremeter measurement, I will handle two objectives: To determine the optimal
sizes of collision spheres for cloth simulation and the required scaling parameters for the cloth to optimally fit the user. It is important that these algorithms do not take more than a thirtieth seconds on a high-end consumer computer in order to keep the real time experience smooth, since there is an averaging over time is involved.

First step will be fitting spheres in various locations in the optimized body
map. These fitted spheres will provide the radii for the collision spheres which
will be used to simulate the cloth. Locations of the machine-provided user
joints are shown in Figure \ref{fig:nite_joints}. These are where spheres will
be located.

\begin{figure}[h]
\centerline{\fbox{\psfig{figure=figures/nite_joints.png,width=1.00\textwidth}}}
\caption{Human Joints provided by NITE }
\label{fig:nite_joints}
\end{figure}

Sphere fitting algorithm will go as following:
\begin{enumerate}
\item Take vector $J_i$ which represents the coordinates of the $i^{th}$
joint.
First, initialize the radius of the sphere by the difference of z-dimension with the overlaying point in the depth map.
\begin{equation}
r_i^z=J_i^z-D^z(J_i^x,J_i^y)
\label{eqn:z_sphere_radius}
\end{equation}
\item Repeat the same process for the x and y dimensions in both negative and positive directions. Take the bigger radius. If there are no points on either side, set it to zero.
\begin{equation}
r_i^{x,y}=max(\| \pm J_i^{x,y} \mp D^{x,y}(J_i^{y,x},J_i^z)\|)
\label{eqn:x_y_sphere_radius}
\end{equation}
\item 3. The radius of the sphere is equal to the minimum of these three values.
\begin{equation}
r_i=min(r_i^{x,y,z})
\label{eqn:minimum_sphere-radius}
\end{equation}
\end{enumerate}

\begin{algorithm}
\dontprintsemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
\KwIn{Optimized Depth Stream From Kinect}
\KwOut{Collision Sphere radii for each joint }
\ForEach{joint }{
$p=pos_{J_m}$\;
$r_z=\sqrt{P_z^2-D_z(P_x,P_y)^2}$
\For{i \bf{from} $P_x$ \bf{to} $0$ }{
\If{$D(i,P_y)$ \bf{equals}  $P_z$} {
  $r_x^- = i$\;
  break\;
 }
}
\For{i \bf{from} $P_x$ \bf{to} $depth_width$ }{
\If{$D(i,P_y)$ \bf{equals}  $P_z$} {
  $r_x^+ = i$\;
  break\;
 }
}
\For{j \bf{from} $P_y$ \bf{to} $0$ }{
\If{$D(P_x,j)$ \bf{equals}  $P_z$} {
  $r_y^- = j$\;
  break\;
 }
}
\For{j \bf{from} $P_y$ \bf{to} $depth_height$ }{
\If{$D(P_x,j)$ \bf{equals}  $P_z$} {
  $r_y^+ = j$\;
  break\;
 }
}
$r_m=min(r_z,r_x^-,r_x^+,r_y^-,r_y^+)$
}
\Return $(r_0,r_1 \ldots r_n)$ 
\caption{Sphere Fitting Algorithm}
\label{algo:sphere_fitting}
\end{algorithm}

\section{Human Body Parameters}
\label{section_8_3}

Next step will be acquiring the optimal scaling parameters for the cloth.
The human body proportions to be used are shown in Table \ref{tbl:human_body_proportions}. In these proportions, the unit width and height are taken as the width and height of the head. The measurements source indicates how the measurement on the user will be performed: Joint Location means algorithm will make use of the joint locations provided by NITE, whereas depth map means the filtered depth map will be used. They can be used together in order to improve the performance. Some of these proportions are not standard enough to be used as references and vary greatly, such as hip width, and will be used directly from measurement.

\begin{table}
\center
\begin{tabular}{ | l | l | l | l |}
\hline
\textbf{Distance} & \textbf{Width} & \textbf{Height} & \textbf{Measure Source} \\ \hline
Head & 1w (1) & 1h (2) & Depth Map+Joint Location \\ \hline
Body Height & - & 7 (3) & Depth Map \\ \hline
Hip Height & - & 4 (4) & Joint Location \\ \hline
Elbow-Fingertip & - & 2 (5) & Depth Map+Joint Location \\ \hline
Wrist to Fingertip & - & 1 (6) & Depth Map+Joint Location \\ \hline
Shoulder Width & 3 (7) & - & Depth Map+Joint Location \\ \hline
Hip Width & - (8) & - & Depth Map \\ \hline
Torso Height & - & - (9) & Joint Location \\ 
\hline
\end{tabular}
\caption{Human Body Proportions \cite{Willis2012}. Numbers in parenthesis represent the lines on Figure \ref{fig:body_proportions}.}
\label{tbl:human_body_proportions}
\end{table}

\begin{figure}[h]
\centerline{\fbox{\psfig{figure=figures/body_proportions.png,width=0.50\textwidth}}}
\caption{Proportions on the Body}
\label{fig:body_proportions}
\end{figure}

Along with the ratios, the actual size in meters in height and width shall be measured and recorded as well, since the cloth needs to be scaled according to the user. In my initial approach, I will scale the whole cloth as a whole, with different parameters for three dimensions. If this approach proves unrealistic, I will process with different scaling parameters for different portions of the cloth, although this would not prove useful in a real fitting room, since most shops do not offer extensive customization.

As the types of cloth focus on different portions of the body, the human body proportions from different areas should not affect the scaling parameters equally. Therefore, the main body parameters for different types of clothes are listed in Table 3.

\begin{table}
\center
\begin{tabular}{ | l | l | l  |}
\hline
\textbf{Type Of Cloth} & \textbf{Primary Height Proportions} & \textbf{Primary Width Proportions}  \\ \hline
Trousers & Hip Height & Hip Width \\ \hline
Long Sleeves & Body Height & Elbow-Fingertip Height,Shoulder Width \\ \hline
Short Sleeves-Sleeveless & Torso Height & Shoulder Width \\ 
\hline
\end{tabular}
\caption{Primary Proportions for Different Cloth Types}
\label{tbl:primary_proportions}
\end{table}

The parameter estimation algorithm will go as following:

\begin{enumerate}
\item For a particular cloth, take the primary measured proportion as $P_i^0$. This will be the measured dimension of said proportion. This process will be repeated for width (W) and height (H).
\item With all the other measured proportions, calculate the estimated value of $P_i$ as $P_i^j$. Here, R denotes the ratio from Table \ref{tbl:human_body_proportions}.
\begin{equation}
W,H_i^j=W,H_j \times R_i^j
\label{eqn:proportion_estimation}
\end{equation}
\item Find the optimized main parameter width as the average:
\begin{equation}
W,H_i=1/(n+1) \times \sum\limits_{j=0}^n W,H_i^j
\label{eqn:optimized_parameter}
\end{equation}
\end{enumerate}

After finding the optimized main parameter in meters, it can be used to scale the virtual cloth by calculating the ratio.

\begin{algorithm}
\dontprintsemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead

$t_{proportion}=import(Table \ref{tbl:human_body_proportions})$ \;
$t_{primary}=import(Table \ref{tbl:primary_proportions})$ \;
$ct=cloth_{type}$\;
$width_{main}=t_{proportion}.width(ct)$\;
$width_{sum}=0$\;
$count_{effector}=0$\;
\ForEach{width \bf{in} $t_{proportion}$ }{
$w_i=measure(p_i)$\;
$w_i^j=w \times t_{proportion}.ratio(p_i,parameter_{main})$\;
$width_{sum}=width_{sum}+w_i^j $\;
$count_effector++ $\;
}
$width_{weighted}=width_{sum}/count_effector$
$x_s=width_{weighted}/width_{cloth}$\;

$height_{main}=t_{proportion}.height(ct)$\;
$height_{sum}=0$\;
$count_{effector}=0$\;
\ForEach{height \bf{in} $t_{proportion}$ }{
$h_i=measure(p_i)$\;
$h_i^j=h \times t_{proportion}.ratio(p_i,parameter_{main})$\;
$height_{sum}=height_{sum}+h_i^j $\;
$count_effector++ $\;
}
$height_{weighted}=height_{sum}/count_effector$
$y_s=height_{weighted}/height_{cloth}$\;
\Return{$(x_s,y_s)$}
\caption{Cloth Resizing Algorithm}
\label{algo:cloth_resize}
\end{algorithm}

