\chapter{Hand Tracking}
\label{appendix_hand_tracking}

\section{OpenCV}
 
The user interface within a natural interaction framework requires functions such as hand state recognition or hand swipe filters, which are omitted in the Kinect for Windows SDK. In order to simulate mouse-clicking behavior, we track the hands of the user to be used as cursors, notice open/close hand gestures for clicking events. In order to implement the algorithms for the proposed hand-track solution, we selected to work with OpenCV, due to its maturity, community and integration with other libraries. OpenCV not only provides basic functions to perform complex and processor-intensive image processing functions such as, facial recognition system, gesture recognition, stereoscopic 3D and segmentation, it also has a very vast machine learning aspect, including boosting, decision three learning and many more features~\cite{opencv_library}.

\section{The Process}

Utilizing such a powerful middleware as a depth sensor, we are able to perform very robust background subtraction with almost no effort. Skeletal body tracking is another embedded property of the software that we use, giving a speed boost. We implemented two different techniques: \em{hand state recognition} and \em{hand swipe recognition}. Hand swipe recognition is simpler compared to hand state recognition. It is just a matter of keeping track of the 3D position of the hand, and keeping a listener that is activated when the 3D velocity of the hand exceeds a certain threshold. We also perform a number of optimizations to make it invariant with the size and location of the user. Open/close hand recognition is harder than hand swipe recognition. We perform image processing in order to determine the state of the hand successfully at relatively low resolutions and large distances. Figure~\ref{fig:hand_recognition_cycle} shows the overview of the hand recognition algorithm. The steps of the algorithm are as follows:

\begin{figure}[h]
\centerline{\fbox{\psfig{figure=figures/hand_recognition_cycle.eps,width=1.0\textwidth}}}
\caption{The overview of the hand recognition algorithm}
\label{fig:hand_recognition_cycle}
\end{figure} 

\begin{enumerate}
\item The size information for the region of interest is drawn from the distance between the user's head and neck.
\item The hand is located using skeletal body tracking.
\item The marked region is copied from the depth stream. 
\item Two dilation and one erosion operations are performed to smooth the hand image.
\item The contour is found on the filtered image.
\item The convex hull of the found contour is calculated.
\item The depth difference between the hull and the actual contour is taken as the reference for hand-state.
\end{enumerate}

\begin{figure}[h]
\centerline{\psfig{figure=figures/open_closed_hands.eps,width=1.0\textwidth}}
\caption{Images and contours of hand regions from the depth stream. Left: open hand and right: closed hand.}
\label{fig:open_closed_hands}
\end{figure}

Figure~\ref{fig:open_closed_hands} depicts the results of experiments for hand recognition. The hand tracking feature is not implemented within the simulation framework yet, as the consumer level usage requires an extensive amount of available apparel meshes as well as better user tracking and motion smoothing. Hence, the user interaction based on hand gestures is left as a future work.

%Ultimately, I plan to implement the algorithm described in~\cite{Tang2011}, where the author claims to have achieved 96\% correct 
%results. I have not included RGB image in the process yet, neither have I implemented complex machine learning. In addition to these 
%methods, I will record hand images for testing objectively. These topics are in my future research plans. 